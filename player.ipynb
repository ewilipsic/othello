{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Othello Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 8\n",
    "\n",
    "def is_valid_move(x, y,board,current_player):\n",
    "    if board[y][x] != 0:\n",
    "        return False\n",
    "    directions = [(0,1),(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1)]\n",
    "    for dx, dy in directions:\n",
    "        if check_direction(x, y, dx, dy,board,current_player):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def check_direction(x, y, dx, dy,board,current_player):\n",
    "    opponent = 1 if current_player == 2 else 2\n",
    "    x, y = x + dx, y + dy\n",
    "    if not (0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE) or board[y][x] != opponent:\n",
    "        return False\n",
    "    while 0 <= x < GRID_SIZE and 0 <= y < GRID_SIZE:\n",
    "        if board[y][x] == 0:\n",
    "            return False\n",
    "        if board[y][x] == current_player:\n",
    "            return True\n",
    "        x, y = x + dx, y + dy\n",
    "    return False\n",
    "\n",
    "def make_move(x, y,board,current_player):\n",
    "    board[y][x] = current_player\n",
    "    directions = [(0,1),(1,1),(1,0),(1,-1),(0,-1),(-1,-1),(-1,0),(-1,1)]\n",
    "    for dx, dy in directions:\n",
    "        if check_direction(x, y, dx, dy,board,current_player):\n",
    "            flip_direction(x, y, dx, dy,board,current_player)\n",
    "\n",
    "def flip_direction(x, y, dx, dy,board,current_player):\n",
    "    x, y = x + dx, y + dy\n",
    "    while board[y][x] != current_player:\n",
    "        board[y][x] = current_player\n",
    "        x, y = x + dx, y + dy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_winner(board,current_player):\n",
    "    black = 0\n",
    "    white = 0\n",
    "    for x in board:\n",
    "        for y in x:\n",
    "            black += (y == 2)\n",
    "            white += (y == 1)\n",
    "    if(black + white != 64) and current_player == 2:\n",
    "        return 0\n",
    "    if(black + white != 64) and current_player == 1:\n",
    "        return 1\n",
    "    return int(black >= 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Player class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player():\n",
    "    def __init__(self,**kwargs):\n",
    "        pass\n",
    "\n",
    "    def get_move(self,board,current_player):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS PLAYER STUFF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "\n",
    "def get_move_random(board,current_player):\n",
    "    valid_moves = []\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            if is_valid_move(x,y,board,current_player): valid_moves.append((x,y))\n",
    "    if(len(valid_moves) == 0): return None    \n",
    "    return valid_moves[random.randint(0,len(valid_moves)-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from copy import deepcopy\n",
    "import time\n",
    "\n",
    "class Node():\n",
    "    def __init__(self,x,y,player_who_decides_next):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.children = []\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "        self.player = player_who_decides_next\n",
    "    \n",
    "    def generate_children(self,board):\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if is_valid_move(x,y,board,self.player): \n",
    "                    if self.player == 1: self.children.append(Node(x,y,2))\n",
    "                    if self.player == 2: self.children.append(Node(x,y,1))\n",
    "        if len(self.children) == 0:\n",
    "            self.children = None\n",
    "\n",
    "    def select(self,C):\n",
    "        if(self.player == 2):\n",
    "            ret = max(self.children,key = lambda child: child.wins/(child.visits+0.001) +  C*( \n",
    "            (2.0*math.log(self.visits))/(child.visits+0.001))**0.5)\n",
    "            return ret\n",
    "        else:\n",
    "            # USING LOSES FOR MINIMIZING PLAYER\n",
    "            ret = max(self.children,key = lambda child: ((child.visits - child.wins)/(child.visits+0.0001) +  C*( \n",
    "            (2.0*math.log(self.visits))/(child.visits+0.0001))**0.5))\n",
    "            return ret\n",
    "\n",
    "    def rollout(self,depth,C,board): # 1 == simulate from children\n",
    "        self.visits += 1\n",
    "\n",
    "        if depth == 0:\n",
    "            ret = self.simulate(board)\n",
    "            self.wins += ret\n",
    "            return ret\n",
    "        \n",
    "        \n",
    "        if self.children == None: \n",
    "            ret = decide_winner(board,self.player)\n",
    "            self.wins += ret\n",
    "            return ret\n",
    "        if len(self.children) == 0: self.generate_children(board)\n",
    "        if self.children == None: \n",
    "            ret = decide_winner(board,self.player)\n",
    "            self.wins += ret\n",
    "            return ret\n",
    "        \n",
    "        \n",
    "        child = self.select(C)\n",
    "        make_move(child.x,child.y,board,self.player)\n",
    "        ret = child.rollout(depth-1,C,board)\n",
    "        self.wins += ret\n",
    "        return ret\n",
    "    \n",
    "    def simulate(self,board):\n",
    "        current_player = self.player\n",
    "        while True:\n",
    "            move = get_move_random(board,current_player)\n",
    "            if(move == None): break\n",
    "            make_move(move[0],move[1],board,current_player)\n",
    "            if current_player == 2: current_player = 1\n",
    "            else: current_player = 2\n",
    "        return decide_winner(board,current_player)\n",
    "    \n",
    "    def give_best_move(self,currentplayer):\n",
    "        if currentplayer == 2 :child = max(self.children,key = lambda child: child.wins/(child.visits+0.001))\n",
    "        else: child = max(self.children,key = lambda child: (child.visits - child.wins)/(child.visits+0.001))\n",
    "        return child.x,child.y\n",
    "\n",
    "class MCTSPlayer(Player):\n",
    "    def __init__(self,explorationFactor = 1.4,rollouts = 100,selectionDepth = 3,timeLimit = 5.0,timed = False):\n",
    "        super().__init__()\n",
    "        self.explorationFactor = explorationFactor\n",
    "        self.rollouts = rollouts\n",
    "        self.selectionDepth = selectionDepth\n",
    "        self.timeLimit = timeLimit\n",
    "        self.timed = timed\n",
    "\n",
    "    def get_move(self,board,current_player):\n",
    "        root = Node(-1,-1,current_player)\n",
    "        if not self.timed:\n",
    "            startTime = time.time()\n",
    "            for i in range(self.rollouts):\n",
    "                sim = deepcopy(board)\n",
    "                root.rollout(self.selectionDepth,self.explorationFactor,sim)\n",
    "            # print(\"ROLL \",time.time() - startTime)\n",
    "        else:\n",
    "            num_rollouts = 0\n",
    "            startTime = time.time()\n",
    "            while time.time() - startTime < self.timeLimit:\n",
    "                num_rollouts+=1\n",
    "                sim = deepcopy(board)\n",
    "                root.rollout(self.selectionDepth,self.explorationFactor,sim)\n",
    "\n",
    "            # print(\"TIME \",time.time() - startTime,\" \",num_rollouts)\n",
    "        return root.give_best_move(current_player)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parrallel MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "\n",
    "import parrallel_utils\n",
    "    \n",
    "class MCTSParallelPlayer(Player):\n",
    "    def __init__(self,explorationFactor = 1.4,rollouts = 800,selectionDepth = 3,num_threads = 4,timed = False,timeLimit = 3.0):\n",
    "        super().__init__()\n",
    "        self.explorationFactor = explorationFactor\n",
    "        self.rollouts = rollouts\n",
    "        self.selectionDepth = selectionDepth\n",
    "        self.num_threads = num_threads\n",
    "        self.timed = timed\n",
    "        self.timeLimit = timeLimit\n",
    "\n",
    "    def get_move(self,board,current_player):\n",
    "        \n",
    "        params = [[deepcopy(board),current_player,self.selectionDepth,self.explorationFactor,self.rollouts//self.num_threads,self.timed,self.timeLimit] for x in range(self.num_threads)]\n",
    "        results = {}\n",
    "        visits = 0\n",
    "     \n",
    "        with concurrent.futures.ProcessPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "            futures = [executor.submit(parrallel_utils.run_mcts_instance,*param) for param in params]\n",
    "            \n",
    "            for future in concurrent.futures.as_completed(futures):\n",
    "            \n",
    "                result_node = future.result()\n",
    "                for child in result_node.children:\n",
    "                    if (child.x,child.y) not in results:\n",
    "                        results[(child.x,child.y)] = {'visits' : 0,'wins' : 0}\n",
    "                    visits += child.visits\n",
    "                    results[(child.x,child.y)]['visits'] += child.visits\n",
    "                    results[(child.x,child.y)]['wins'] += child.wins\n",
    "\n",
    "        # print(visits)\n",
    "        values  = []\n",
    "        moves = []\n",
    "        if(current_player == 2):\n",
    "            for x,y in results.items():\n",
    "                values.append(y['wins']/(y['visits'] + 0.001))\n",
    "                moves.append(x)\n",
    "        if(current_player == 1):\n",
    "            for x,y in results.items():\n",
    "                values.append(y['visits'] - y['wins']/(y['visits'] + 0.001))\n",
    "                moves.append(x)\n",
    "\n",
    "        maxi = 0\n",
    "        for i in range(len(values)):\n",
    "            if values[maxi] < values[i] : maxi = i\n",
    "\n",
    "        return moves[maxi][0],moves[maxi][1]\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALPHA-BETA PLAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBetaPlayer(Player):\n",
    "    \n",
    "    def __init__(self,depth = 2):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "    \n",
    "    def naiveValue(self,board):\n",
    "        val = 0\n",
    "        for x in board:\n",
    "            for y in x:\n",
    "                if y == 2: val += 1\n",
    "                if y == 1: val -= 1\n",
    "        return val\n",
    "\n",
    "    def minimax(self,board,depth,alpha,beta,maximizingPlayer,current_player):\n",
    "    \n",
    "        valid_moves = []\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if is_valid_move(x,y,board,current_player): valid_moves.append((x,y))\n",
    "\n",
    "        if depth == 0: \n",
    "            return self.naiveValue(board)\n",
    "\n",
    "        if len(valid_moves) == 0: return decide_winner(board,current_player) * 64\n",
    "\n",
    "        if maximizingPlayer:\n",
    "            value = -100\n",
    "            for x,y in valid_moves:\n",
    "            \n",
    "                child = deepcopy(board)\n",
    "                make_move(x,y,child,2)\n",
    "\n",
    "                value = max(value,self.minimax(child,depth-1,alpha,beta,False,1))\n",
    "                if value > beta:\n",
    "                    break\n",
    "                alpha = max(alpha,value)\n",
    "            return value\n",
    "        else:\n",
    "            value = 100\n",
    "            for x,y in valid_moves:\n",
    "\n",
    "                child = deepcopy(board)\n",
    "                make_move(x,y,child,1)\n",
    "\n",
    "                value = min(value,self.minimax(child,depth-1,alpha,beta,True,2))\n",
    "                if value < alpha:\n",
    "                    break\n",
    "                beta = min(beta,value)\n",
    "            return value\n",
    "\n",
    "    def get_move(self,board,current_player):\n",
    "        if(current_player == 2):\n",
    "            valid_moves = []\n",
    "            for x in range(GRID_SIZE):\n",
    "                for y in range(GRID_SIZE):\n",
    "                    if is_valid_move(x,y,board,2): valid_moves.append((x,y))\n",
    "    \n",
    "            points = []\n",
    "            for x,y in valid_moves:\n",
    "            \n",
    "                sim = deepcopy(board)\n",
    "                make_move(x,y,sim,2)\n",
    "    \n",
    "                points.append(self.minimax(sim,self.depth,-1000,+1000,False,1))\n",
    "    \n",
    "            maxi = 0\n",
    "            for i in range(len(valid_moves)):\n",
    "                if points[i] > points[maxi]:\n",
    "                    maxi = i\n",
    "    \n",
    "            return valid_moves[maxi]\n",
    "        else:\n",
    "            valid_moves = []\n",
    "            for x in range(GRID_SIZE):\n",
    "                for y in range(GRID_SIZE):\n",
    "                    if is_valid_move(x,y,board,1): valid_moves.append((x,y))\n",
    "    \n",
    "            points = []\n",
    "            for x,y in valid_moves:\n",
    "            \n",
    "                sim = deepcopy(board)\n",
    "                make_move(x,y,sim,1)\n",
    "    \n",
    "                points.append(self.minimax(sim,self.depth,-1000,+1000,True,2))\n",
    "    \n",
    "            mini = 0\n",
    "            for i in range(len(valid_moves)):\n",
    "                if points[i] < points[mini]:\n",
    "                    mini = i\n",
    "    \n",
    "            return valid_moves[mini]\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPlayer(Player):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def get_move(self, board, current_player):\n",
    "        valid_moves = []\n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if is_valid_move(x,y,board,current_player): valid_moves.append((x,y))\n",
    "        return valid_moves[random.randint(0,len(valid_moves)-1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Value Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "class PolicyValueNetwork(nn.Module):\n",
    "    def __init__(self, grid_size):\n",
    "        super().__init__()\n",
    "        self.grid_size = grid_size\n",
    "        \n",
    "        # Convolutional layers for board processing\n",
    "        self.conv1 = nn.Conv2d(2, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        \n",
    "        # Policy head - predicts move probabilities\n",
    "        self.policy_conv = nn.Conv2d(128, 2, 1)\n",
    "        self.policy_fc = nn.Linear(2 * grid_size * grid_size, grid_size * grid_size)\n",
    "        \n",
    "        # Value head - predicts game outcome\n",
    "        self.value_conv = nn.Conv2d(128, 1, 1)\n",
    "        self.value_fc1 = nn.Linear(grid_size * grid_size, 64)\n",
    "        self.value_fc2 = nn.Linear(64, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Common representation\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.relu(self.conv3(x))\n",
    "        \n",
    "        # Policy output\n",
    "        policy = torch.relu(self.policy_conv(x))\n",
    "        policy = policy.view(-1, 2 * self.grid_size * self.grid_size)\n",
    "        policy = self.policy_fc(policy)\n",
    "        policy = torch.softmax(policy, dim=1)\n",
    "        \n",
    "        # Value output\n",
    "        value = torch.relu(self.value_conv(x))\n",
    "        value = value.view(-1, self.grid_size * self.grid_size)\n",
    "        value = torch.relu(self.value_fc1(value))\n",
    "        value = torch.tanh(self.value_fc2(value))\n",
    "        value = (value + 1)/2\n",
    "        \n",
    "        return policy, value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def board_to_tensor(board):\n",
    "    tensor = torch.zeros(2, GRID_SIZE, GRID_SIZE)\n",
    "    \n",
    "    for i in range(GRID_SIZE):\n",
    "        for j in range(GRID_SIZE):\n",
    "            if board[i][j] == 1:\n",
    "                tensor[0, i, j] = 1\n",
    "            elif board[i][j] == 2:\n",
    "                tensor[1, i, j] = 1\n",
    "    \n",
    "    return tensor.unsqueeze(0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLNode(Node):\n",
    "    def __init__(self, x, y, player_who_decides_next, prior_prob=0):\n",
    "        super().__init__(x, y, player_who_decides_next)\n",
    "        self.prior_prob = prior_prob\n",
    "        self.children = []\n",
    "    \n",
    "    def select(self, C):\n",
    "        # AlphaZero-style selection with prior probabilities\n",
    "        if self.player == 2:\n",
    "            ret = max(self.children, key=lambda child: \n",
    "                 child.wins / (child.visits + 0.001) + \n",
    "                 C * child.prior_prob * ((self.visits) ** 0.5) / (1 + child.visits))\n",
    "        else:\n",
    "            ret = max(self.children, key=lambda child: \n",
    "                 (child.visits - child.wins) / (child.visits + 0.001) + \n",
    "                 C * child.prior_prob * ((self.visits) ** 0.5) / (1 + child.visits))\n",
    "        return ret\n",
    "    \n",
    "    def generate_children(self, board, policy_network):\n",
    "        # Get policy from neural network\n",
    "        board_tensor = board_to_tensor(board)\n",
    "        policy, _ = policy_network(board_tensor)\n",
    "        policy = policy.detach().numpy().reshape(GRID_SIZE, GRID_SIZE)\n",
    "        \n",
    "        for x in range(GRID_SIZE):\n",
    "            for y in range(GRID_SIZE):\n",
    "                if is_valid_move(x, y, board, self.player):\n",
    "                    prior_prob = policy[x][y]\n",
    "                    if self.player == 1:\n",
    "                        self.children.append(RLNode(x, y, 2, prior_prob))\n",
    "                    if self.player == 2:\n",
    "                        self.children.append(RLNode(x, y, 1, prior_prob))\n",
    "        \n",
    "        if len(self.children) == 0:\n",
    "            self.children = None\n",
    "    \n",
    "    def rollout(self, depth, C, board, policy_value_network):\n",
    "        self.visits += 1\n",
    "        \n",
    "\n",
    "        if self.children == None:\n",
    "            ret = decide_winner(board, self.player)\n",
    "            self.wins += ret\n",
    "            return ret\n",
    "        \n",
    "     \n",
    "        if len(self.children) == 0:\n",
    "            board_tensor = board_to_tensor(board)\n",
    "            _, value = policy_value_network(board_tensor)\n",
    "            value = value.item()\n",
    "            \n",
    "            self.generate_children(board, policy_value_network)\n",
    "            \n",
    "            if self.children == None:\n",
    "                ret = decide_winner(board, self.player)\n",
    "            else:\n",
    "                ret = value\n",
    "            \n",
    "            self.wins += ret\n",
    "            return ret\n",
    "\n",
    "        child = self.select(C)\n",
    "        make_move(child.x, child.y, board, self.player)\n",
    "        ret = child.rollout(depth - 1, C, board, policy_value_network)\n",
    "        self.wins += ret\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_game_over(board,current_player):\n",
    "    for x in range(GRID_SIZE):\n",
    "        for y in range(GRID_SIZE):\n",
    "            if is_valid_move(x,y,board,current_player): return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RLMCTSPlayer(Player):\n",
    "    def __init__(self, explorationFactor=1.4, rollouts=400, selectionDepth=4, timeLimit = 3.0,timed = False,checkpointNUM = 0):\n",
    "        super().__init__()\n",
    "        self.policy_value_network = PolicyValueNetwork(GRID_SIZE)\n",
    "        self.optimizer = optim.Adam(self.policy_value_network.parameters(), lr=0.001)\n",
    "        self.explorationFactor = explorationFactor\n",
    "        self.rollouts = rollouts\n",
    "        self.selectionDepth = selectionDepth\n",
    "        self.timeLimit = timeLimit\n",
    "        self.timed = timed\n",
    "        self.checkpointNUM = checkpointNUM\n",
    "    \n",
    "    def get_move(self, board, current_player):\n",
    "        root = RLNode(-1, -1, current_player)\n",
    "        if not self.timed:\n",
    "            for i in range(self.rollouts):\n",
    "                sim = deepcopy(board)\n",
    "                root.rollout(self.selectionDepth, self.explorationFactor, sim, self.policy_value_network)\n",
    "        else:\n",
    "            num_rollouts = 0\n",
    "            startTime = time.time()\n",
    "            while time.time() - startTime < self.timeLimit:\n",
    "                num_rollouts += 1\n",
    "                sim = deepcopy(board)\n",
    "                root.rollout(self.selectionDepth,self.explorationFactor,sim,self.policy_value_network)\n",
    "\n",
    "        return root.give_best_move(current_player)\n",
    "    \n",
    "    def train(self, game_states, moves, outcomes):\n",
    "        \"\"\"Train the network using game data\"\"\"\n",
    "        self.policy_value_network.train()\n",
    "        \n",
    "        for state, move, outcome in zip(game_states, moves, outcomes):\n",
    "            state_tensor = board_to_tensor(state)\n",
    "            policy, value = self.policy_value_network(state_tensor)\n",
    "            \n",
    "            # Create target policy (one-hot encoding of the move)\n",
    "            target_policy = torch.zeros_like(policy)\n",
    "            move_idx = move[0] * GRID_SIZE + move[1]\n",
    "            target_policy[0, move_idx] = 1\n",
    "            \n",
    "            # Calculate losses\n",
    "            value_loss = nn.MSELoss()(value, torch.tensor([[outcome]], dtype=torch.float))\n",
    "            policy_loss = -torch.sum(target_policy * torch.log(policy + 1e-8))\n",
    "            loss = value_loss + policy_loss\n",
    "            \n",
    "            # Update network\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "    \n",
    "    def self_play(self, num_games=20):\n",
    "        \"\"\"Generate training data through self-play\"\"\"\n",
    "        game_states = []\n",
    "        moves = []\n",
    "        outcomes = []\n",
    "        \n",
    "        for _ in range(num_games):\n",
    "            board = [[0 for _ in range(GRID_SIZE)] for _ in range(GRID_SIZE)]\n",
    "            board[3][4] = board[4][3] = 2  # White\n",
    "            board[3][3] = board[4][4] = 1  # Black\n",
    "            current_player = 1\n",
    "            states = []\n",
    "            actions = []\n",
    "            \n",
    "            while True:\n",
    "                # Store current state\n",
    "                states.append(deepcopy(board))\n",
    "                \n",
    "                # Get move\n",
    "                move = self.get_move(board, current_player)\n",
    "                actions.append(move)\n",
    "                \n",
    "                # Make move\n",
    "                make_move(move[0], move[1], board, current_player)\n",
    "                \n",
    "                # Check if game is over\n",
    "                current_player = 3 - current_player \n",
    "                \n",
    "                if is_game_over(board,current_player):  \n",
    "                    winner = decide_winner(board, current_player)\n",
    "                    \n",
    "                    # Assign outcome to each state-action pair\n",
    "                    for state, action in zip(states, actions):\n",
    "                        game_states.append(state)\n",
    "                        moves.append(action)\n",
    "                        outcomes.append(winner)\n",
    "                    break\n",
    "                \n",
    "        # Train network with generated data\n",
    "        self.train(game_states, moves, outcomes)\n",
    "        torch.save(self.policy_value_network.state_dict(), 'RLMCTS_5SEC_1.4/checkpoint' + str(self.checkpointNUM) + '.pth')\n",
    "        self.checkpointNUM+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
